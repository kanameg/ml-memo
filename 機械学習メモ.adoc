:lang: ja
:doctype: book

:chapter-label:
:sectnums:

= 機械学習ノート

== 特徴量の生成

=== 特徴量とは
機械学習で学習や推測にもちるデータの種類のことを特徴量という。
機械学習で用いるモデルは数値データのみしか入力できない（文字や文字列データは入力不可）ため、入力する元のデータの特徴量は数値化する必要がある。
また選択したモデルによって数値の取りか使い方が異なるため、モデルによって特徴量の数値の意味や大小関係などを考慮する必要がある。
例えば、GBDT(Gradient Boosting Decision Tree)では、決定木が使われているため以下のような特徴がある。

* 数値の大きさにには意味がない、数値の大小関係に意味がある
* 欠損値があっても、そのまま扱うことができる

数値の大きさに意味がないため、数値データの範囲や分布を気にする必要があまりない。また欠損値がそのままでもある程度問題がないため非常に扱いやすいモデルとも言える。
もちろん欠損値は埋められるときには埋めた方が精度向上する場合もある。分布やデータの範囲も気にした方が精度向上する場合もある。
それに対して、NN(ニューラルネットワーク)では、以下のような特徴がある。

* 数値に意味があるので、大きさに影響される
* 欠損値を取り扱えないので欠損値を処理する必要がある

また、線形モデルなどでは特徴量の関係が線形でないと精度が出ないモデルの場合、対数を取ったりするなど非線形な変換をする必要がある。

=== 欠損値とは
データ生成時の不備や、そもそも値が存在しない（データ数が0の場合の平均値）などにより欠損したデータのことを欠損値という。

==== 欠損値の扱い
欠損値の扱いとしては以下の方法が考えられる。

===== 欠損値をそのままで扱う
GBDTなどのモデルでは欠損値をそのままで扱うことも可能。その場合は欠損値をそのままにしておいてもよい。
ただし補完できる場合は推論精度が向上する場合もあるので、両方を試した方がよい場合もある。

scikit-learnのランダムフォレストなどは欠損値を扱えないため、-9999などの範囲外の値で補完することにより欠損値のまま扱うことも可能。

===== 欠損値を代表値で補完
欠損発生の理由が偶然の場合やランダムである場合は、代表値で補完した方が精度向上する場合もある。逆にランダムでない場合は、欠損であることに意味のある場合があるので、補完をしないほうが良い。

代表値で補完する場合は、平均値や中央値などを利用する。
またグループ毎で代表値が異なりそうな場合は、別のカテゴリ変数でグループを作りグループ毎の平均をとって補完したりする。

欠損している値がカテゴリ変数の場合は、データ全体やグループ毎で最も多い値(mode)へ置き換えるなどの方法がある。

===== 欠損値を予測
欠損している変数が他の変数と関連性がある場合は、他の変数から予測することで補完が可能。

欠損値のある変数を目的変数として、その他を特徴量として予測モデルを作成する。
この際に本来の目的変数を特徴量に入れるとテストデータの方の補完ができないため特徴量に入れない。
そのため、テストデータも学習データとして使用してもよい。作成したモデルで欠損値のデータを予測して補完する。

===== 欠損値から新たな特徴量を生成
欠損値がランダムに発生しているわけではない場合、欠損値から新たな特徴量を生成可能な場合がある。
欠損であったという2値情報を生成することで欠損情報を保存できる（欠損値自体は補完することも可能）。
レコードごとに欠損値の数をカウントし、その数を特徴量とすることも可能。

==== 欠損値の補完
欠損値を補完する場合、Pandasでは、欠損値を `NaN` にしておくことで `fillna` 関数を利用することで補完可能。

[source, python]
----
df['col'] = df['col1'].fillna(0)  # 0で補完
df['col'] = df['col1'].fillna(df['col1'].mean())  # 平均値で補完
----


== 数値変数

=== 数値変数の変換
数値変数は基本的にそのままモデルに入力が可能だが、数値の範囲や分布を変換したほうが、モデルを学習しやすくなる場合がある。

==== 標準化(standalization)
線形変換の1つ（加算と乗算のみの変換）で変数の平均を0、標準偏差を1に変換する操作。
線形回帰やロジスティック回帰などでは、変数のスケールが大きいほど求める係数が小さくなり、学習がうまくいかなくなる傾向になる
（係数の差が小さくなるため）。NNにおいても変数同士のスケール差が大きいと、うまく学習を進められないことが多い。

2値変数については、0と1の絶対値を変換されてしまう場合があるので、標準化を行わない方がよい。

Pythonではscikit-learnのStandardScalerクラスを利用することで変換することが可能。

[source, python]
----
from sklearn.preprocessing import StandardScaler

# 学習データに基づいて複数列の標準化を定義
scaler = StandardScaler()
scaler.fit(train_x[num_cols])

# 変換後のデータで各列を置換
train_x[num_cols] = scaler.transform(train_x[num_cols])
test_x[num_cols] = scaler.transform(test_x[num_cols])
----

学習データとテストデータを使って標準化を行うか、学習データのみを使って標準化を行うかは基本的にはどちらでも大差はない。
実務上は学習データしか手に入らないので学習データのみで行う場合が多い。学習データのみで行うと過学習してしまうのでテストデータも含めて
標準化するという考え方もある。


==== Min-Maxスケーリング

標準化と同様に変数のスケールを揃える方法で、変数の取り得ている範囲を0から1などの特定の区間におさめる方法。
変換後の平均が0にならない、外れ値の影響を顕著に受けるなどの問題がある。範囲が決まっている値の場合は使う場合がある。

Pythonではscikit-learnのMinMaxScalerクラスを利用することで変換することが可能。

[source, python]
----
from sklearn.preprocessing import MinMaxScaler

# 学習データに基づいて複数列のMin-Maxスケーリングを定義
scaler = MinMaxScaler()
scaler.fit(train_x[num_cols])

# 変換後のデータで各列を置換
train_x[num_cols] = scaler.transform(train_x[num_cols])
test_x[num_cols] = scaler.transform(test_x[num_cols])
----

==== 非線形変換
正規分布ではない分布を正規分布に近い形へ変換するなど、
変数の分布の形を変えたい場合は、非線形の変換を行なうほうが効果的である。

金額などの分布に偏りが出やすい変数には、対数変換を行なった方が効果的である。
ただし値が0をとりうる場合は0の対数は取れないため、`log(x+1)` で変換をする。
また負の値を取りうる場合は、絶対値の対数を取り結果に符号を付加する場合もある。

[source, python]
----
# 単に対数をとる
x1 = np.log(x)

# 1を加えたあとに対数をとる
x2 = np.log1p(x)

# 絶対値の対数をとってから元の符号を付加する
x3 = np.sign(x) * np.log(np.abs(x))
----

=== Box-Cox変換
対数変換を一般化した変換方法としてBox-Cox変換がある。
scikit-learnの変換を利用することで、自動的に正規分布へ近い形に変換してくることができる。


== カテゴリ変数

GBDTのような決定木を利用するような場合は、ラベルエンコーディングが最も一般的な方法。

=== one-hotエンコード
最も代表的なカテゴリ変数に対する数値変換方法。カテゴリ変数に対して、その値に該当するかどうかを0,1で表す2値変数に変換する。
そのためnこの状態をもつカテゴリ変数にone-hotエンコードを行うとnこのダミー変数が作成される。
カテゴリ変数の状態が多い変数にはちょっと使いにくい。

多重共線性を気にしてn個のダミー変数ではなく、n-1個のダミー変数にする場合があるが、
GBDTなどのモデルは多重共線性が問題にならないのと、正則化を行っているためn個のままon-hotエンコードを行っても問題がない。

PythonではPandasでもできるが、
以下のようにlink:https://contrib.scikit-learn.org/category_encoders/onehot.html[category_encoders.OneHotEncoder]
を使った方が楽にできる。 

[source, python]
----
import category_encoders as ce

# OneHotEncodeしたい列を指定。Nullや不明の場合の補完方法も指定。
ce_ohe = ce.OneHotEncoder(cols=['device'], handle_unknown='impute')

# pd.DataFrameをそのまま突っ込む
df_onehot = ce_ohe.fit_transform(df)

df_onehot.head()
----


=== ラベルエンコード
ラベルエンコードは単純に水準値を0からの整数に置き換える。大小関係に意味がないため、ラベルエンコードしたものを決定木以外のモデルで
学習に用いるのは適切ではない。ただし決定木では特定の水準のみが目的変数に影響がある場合に分岐を繰り返すことができるので
（例えば、値4が目的変数に影響がある場合4より大きい小さいで分岐することができる）学習に用いることができる。

ラベルエンコード(Label Encode)はPanadasのクラス名からきており、順序エンコード(Ordinal Encode)という方が一般的である。

PythonではPandasでもできるが、
以下のようにlink:https://contrib.scikit-learn.org/category_encoders/ordinal.html[category_encoders.OrdinalEncoder]
を使った方が楽にできる。

[source, python]
----
import category_encoders as ce

# 序数をカテゴリに付与して変換
ce_oe = ce.OrdinalEncoder(cols=['device'], handle_unknown='impute')
df_ordinal = ce_oe.fit_transform(df)

df_ordinal.head()
----

=== フィーチャハッシング
フィーチャーハッシング(Feature Hashing)は、one-hotエンコードでは水準数の数によって大量のダミー変数が作成されるため、
ダミー変数の数を設定で限定しハッシュを用いることで2値のビットを立てる位置を変更します。
そのため異なる水準値にもかかわらず、同じビットに1が立つ場合があります。

GBDTなどの決定木のモデルでは、このエンコードを利用するよりも、ラベルエンコードを使った方が簡単だし
良い性能が得られる場合が多い。

pythonでは、link:https://contrib.scikit-learn.org/category_encoders/hashing.html[category_encoders.HashingEncoder]を使うと簡単に変換可能。

[source, python]
----
import category_encoders as ce

# FeatureHashingしたい列を指定。Nullや不明の場合の補完方法も指定。
ce_hse = ce.Hashing(cols=['device'], handle_unknown='impute')

# pd.DataFrameをそのまま突っ込む
df_hash = ce_hse.fit_transform(df)

df_hash.head()
----

=== frequency encoding

=== target encoding
