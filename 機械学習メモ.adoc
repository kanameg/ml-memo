:lang: ja
:doctype: book

:chapter-label:
:sectnums:

= 機械学習ノート

== 特徴量の生成

=== 特徴量とは
機械学習で学習や推測にもちるデータの種類のことを特徴量という。
機械学習で用いるモデルは数値データのみしか入力できない（文字や文字列データは入力不可）ため、入力する元のデータの特徴量は数値化する必要がある。
また選択したモデルによって数値の取りか使い方が異なるため、モデルによって特徴量の数値の意味や大小関係などを考慮する必要がある。
例えば、GBDT(Gradient Boosting Decision Tree)では、決定木が使われているため以下のような特徴がある。

* 数値の大きさにには意味がない、数値の大小関係に意味がある
* 欠損値があっても、そのまま扱うことができる

数値の大きさに意味がないため、数値データの範囲や分布を気にする必要があまりない。また欠損値がそのままでもある程度問題がないため非常に扱いやすいモデルとも言える。
もちろん欠損値は埋められるときには埋めた方が精度向上する場合もある。分布やデータの範囲も気にした方が精度向上する場合もある。
それに対して、NN(ニューラルネットワーク)では、以下のような特徴がある。

* 数値に意味があるので、大きさに影響される
* 欠損値を取り扱えないので欠損値を処理する必要がある

また、線形モデルなどでは特徴量の関係が線形でないと精度が出ないモデルの場合、対数を取ったりするなど非線形な変換をする必要がある。

=== 欠損値とは
データ生成時の不備や、そもそも値が存在しない（データ数が0の場合の平均値）などにより欠損したデータのことを欠損値という。

==== 欠損値の扱い
欠損値の扱いとしては以下の方法が考えられる。

===== 欠損値をそのままで扱う
GBDTなどのモデルでは欠損値をそのままで扱うことも可能。その場合は欠損値をそのままにしておいてもよい。
ただし補完できる場合は推論精度が向上する場合もあるので、両方を試した方がよい場合もある。

scikit-learnのランダムフォレストなどは欠損値を扱えないため、-9999などの範囲外の値で補完することにより欠損値のまま扱うことも可能。

===== 欠損値を代表値で補完
欠損発生の理由が偶然の場合やランダムである場合は、代表値で補完した方が精度向上する場合もある。逆にランダムでない場合は、欠損であることに意味のある場合があるので、補完をしないほうが良い。

代表値で補完する場合は、平均値や中央値などを利用する。
またグループ毎で代表値が異なりそうな場合は、別のカテゴリ変数でグループを作りグループ毎の平均をとって補完したりする。

欠損している値がカテゴリ変数の場合は、データ全体やグループ毎で最も多い値(mode)へ置き換えるなどの方法がある。

===== 欠損値を予測
欠損している変数が他の変数と関連性がある場合は、他の変数から予測することで補完が可能。

欠損値のある変数を目的変数として、その他を特徴量として予測モデルを作成する。
この際に本来の目的変数を特徴量に入れるとテストデータの方の補完ができないため特徴量に入れない。
そのため、テストデータも学習データとして使用してもよい。作成したモデルで欠損値のデータを予測して補完する。

===== 欠損値から新たな特徴量を生成
欠損値がランダムに発生しているわけではない場合、欠損値から新たな特徴量を生成可能な場合がある。
欠損であったという2値情報を生成することで欠損情報を保存できる（欠損値自体は補完することも可能）。
レコードごとに欠損値の数をカウントし、その数を特徴量とすることも可能。

==== 欠損値の補完
欠損値を補完する場合、Pandasでは、欠損値を `NaN` にしておくことで `fillna` 関数を利用することで補完可能。

```
df['col'] = df['col1'].fillna(0)  # 0で補完
df['col'] = df['col1'].fillna(df['col1'].mean())  # 平均値で補完
```

== 数値変数

=== 数値変数の変換
数値変数は基本的にそのままモデルに入力が可能だが、数値の範囲や分布を変換したほうが、モデルを学習しやすくなる場合がある。

==== 標準化(standalization)
線形変換の1つ（加算と乗算のみの変換）で変数の平均を0、標準偏差を1に変換する操作。
線形回帰やロジスティック回帰などでは、変数のスケールが大きいほど求める係数が小さくなり、学習がうまくいかなくなる傾向になる
（係数の差が小さくなるため）。NNにおいても変数同士のスケール差が大きいと、うまく学習を進められないことが多い。

2値変数については、0と1の絶対値を変換されてしまう場合があるので、標準化を行わない方がよい。

Pythonではscikit-learnのStandardScalerクラスを利用することで変換することが可能。

```Python
from sklearn.preprocessing import StandardScaler

# 学習データに基づいて複数列の標準化を定義
scaler = StandardScaler()
scaler.fit(train_x[num_cols])

# 変換後のデータで各列を置換
train_x[num_cols] = scaler.transform(train_x[num_cols])
test_x[num_cols] = scaler.transform(test_x[num_cols])
```

学習データとテストデータを使って標準化を行うか、学習データのみを使って標準化を行うかは基本的にはどちらでも大差はない。
実務上は学習データしか手に入らないので学習データのみで行う場合が多い。学習データのみで行うと過学習してしまうのでテストデータも含めて
標準化するという考え方もある。


==== Min-Maxスケーリング

標準化と同様に変数のスケールを揃える方法で、変数の取り得ている範囲を0から1などの特定の区間におさめる方法。
変換後の平均が0にならない、外れ値の影響を顕著に受けるなどの問題がある。範囲が決まっている値の場合は使う場合がある。

Pythonではscikit-learnのMinMaxScalerクラスを利用することで変換することが可能。
```python
from sklearn.preprocessing import MinMaxScaler

# 学習データに基づいて複数列のMin-Maxスケーリングを定義
scaler = MinMaxScaler()
scaler.fit(train_x[num_cols])

# 変換後のデータで各列を置換
train_x[num_cols] = scaler.transform(train_x[num_cols])
test_x[num_cols] = scaler.transform(test_x[num_cols])
```

==== 非線形変換
正規分布ではない分布を正規分布に近い形へ変換するなど、
変数の分布の形を変えたい場合は、非線形の変換を行なうほうが効果的である。

金額などの分布に偏りが出やすい変数には、対数変換を行なった方が効果的である。
ただし値が0をとりうる場合は0の対数は取れないため、`log(x+1)` で変換をする。
また負の値を取りうる場合は、絶対値の対数を取り結果に符号を付加する場合もある。

```python
# 単に対数をとる
x1 = np.log(x)

# 1を加えたあとに対数をとる
x2 = np.log1p(x)

# 絶対値の対数をとってから元の符号を付加する
x3 = np.sign(x) * np.log(np.abs(x))
```

=== Box-Cox変換
対数変換を一般化した変換方法としてBox-Cox変換がある。
scikit-learnの変換を利用することで、自動的に正規分布へ近い形に変換してくることができる。


== カテゴリ変数

GBDTのような決定木を利用するような場合は、ラベルエンコーディングが最も一般的な方法。

=== one hot encodeing
このコードが辺なのがいい。

=== label encoding

=== feature hashing

=== frequency encoding

=== target encoding

